{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Exercises - Conceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "- (a) better - a more flexible approach will fit the data closer and with the large sample size a better fit than an inflexible approach would be obtained\n",
    "\n",
    "- (b) worse - a flexible method would overfit the small number of observations\n",
    "\n",
    "- (c) better - with more degrees of freedom, a flexible model would obtain a better fit\n",
    "\n",
    "- (d) worse - flexible methods fit to the noise in the error terms and increase variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\n",
    "- (a) better - a more flexible approach will fit the data closer and with the large sample size a better fit than an inflexible approach would be obtained\n",
    "\n",
    "- (b) worse - a flexible method would overfit the small number of observations\n",
    "\n",
    "- (c) better - with more degrees of freedom, a flexible model would obtain a better fit\n",
    "\n",
    "- (d) worse - flexible methods fit to the noise in the error terms and increase variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\n",
    "\n",
    "- (a)\n",
    "\n",
    "  <img src=\"../../../images/2.4 Exercises - Conceptual - 3.a.jpg\">\n",
    "\n",
    "\n",
    "- (b)\n",
    "\n",
    "  - Bias (squared) - decreases monotonically because increases in flexibility yield a closer fit\n",
    "\n",
    "  - Variance - increases monotonically because increases in flexibility yield overfit\n",
    "\n",
    "  - Training error - decreases monotonically because increases in flexibility yield a closer fit\n",
    "\n",
    "  - Test error - concave up curve because increase in flexibility yields a closer fit before it overfits\n",
    "\n",
    "  - Bayes (irreducible) error - defines the lower limit, the test error is bounded below by the irreducible error due to variance in the error (epsilon) in the output values (0 <= value). When the training error is lower than the irreducible error, overfitting has taken place.  \n",
    "  The Bayes error rate is defined for classification problems and is determined by the ratio of data points which lie at the 'wrong' side of the decision boundary, (0 <= value < 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\n",
    "\n",
    "- (a)\n",
    "\n",
    "  - i. stock market price direction, prediction, response: up, down, input: yesterday's price movement % change, two previous day price movement % change, etc.\n",
    "\n",
    "  - ii. illness classification, inference, response: ill, healthy, input: resting heart rate, resting breath rate, mile run time\n",
    "\n",
    "  - iii. car part replacement, prediction, response: needs to be replace, good, input: age of part, mileage used for, current amperage\n",
    "  \n",
    "\n",
    "- (b)\n",
    "\n",
    "  - i. CEO salary. inference. predictors: age, industry experience, industry, years of education. response: salary.\n",
    "\n",
    "  - ii. car part replacement. inference. response: life of car part. predictors: age of part, mileage used for, current amperage.\n",
    "\n",
    "  - iii. illness classification, prediction, response: age of death, input: current age, gender, resting heart rate, resting breath rate, mile run time.\n",
    "  \n",
    "\n",
    "- (c)\n",
    "\n",
    "  - i. cancer type clustering. diagnose cancer types more accurately.\n",
    "\n",
    "  - ii. Netflix movie recommendations. recommend movies based on users who have watched and rated similar movies.\n",
    "\n",
    "  - iii. marketing survey. clustering of demographics for a product(s) to see which clusters of consumers buy which products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\n",
    "\n",
    "The advantages for a very flexible approach for regression or classification are obtaining a better fit for non-linear models, decreasing bias.\n",
    "\n",
    "The disadvantages for a very flexible approach for regression or classification are requires estimating a greater number of parameters, follow the noise too closely (overfit), increasing variance.\n",
    "\n",
    "A more flexible approach would be preferred to a less flexible approach when we are interested in prediction and not the interpretability of the results.\n",
    "\n",
    "A less flexible approach would be preferred to a more flexible approach when we are interested in inference and the interpretability of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\n",
    "\n",
    "A parametric approach reduces the problem of estimating f down to one of estimating a set of parameters because it assumes a form for f.\n",
    "\n",
    "A non-parametric approach does not assume a functional form for f and so requires a very large number of observations to accurately estimate f.\n",
    "\n",
    "The advantages of a parametric approach to regression or classification are the simplifying of modeling f to a few parameters and not as many observations are required compared to a non-parametric approach.\n",
    "\n",
    "The disadvantages of a parametric approach to regression or classification are a potential to inaccurately estimate f if the form of f assumed is wrong or to overfit the observations if more flexible models are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\n",
    "\n",
    "- (a)\n",
    "\n",
    "| Obs.  | X1  | X2  | X3  | Distance(0, 0, 0)  | Y     |\n",
    "|:-----:|:---:|:---:|:---:|:------------------ |:----- |\n",
    "|   1   |  0  |  3  |  0  | 3                  | Red   |\n",
    "|   2   |  2  |  0  |  0  | 2                  | Red   |\n",
    "|   3   |  0  |  1  |  3  | sqrt(10) ~ 3.2     | Red   |\n",
    "|   4   |  0  |  1  |  2  | sqrt(5) ~ 2.2      | Green |\n",
    "|   5   | -1  |  0  |  1  | sqrt(2) ~ 1.4      | Green |\n",
    "|   6   |  1  |  1  |  1  | sqrt(3) ~ 1.7      | Red   |      \n",
    "\n",
    "- (b) Green. Observation #5 is the closest neighbor for K = 1.\n",
    "\n",
    "- (c) Red. Observations #2, 5, 6 are the closest neighbors for K = 3. 2 is Red, 5 is Green, and 6 is Red.\n",
    "\n",
    "- (d) Small. A small K would be flexible for a non-linear decision boundary, whereas a large K would try to fit a more linear boundary because it takes more points into consideration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
